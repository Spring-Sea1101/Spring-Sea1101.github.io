# 梯度搜索

[TOC]

## 一 原理

* *通过计算函数的梯度（即导数），沿着梯度的反方向更新参数，以逐步接近或达到最优值。*



### 1. 两个数学概念

#### （1） 梯度

* 在多元函数中，梯度是一个==向量==，它的方向==指向====函数在某一点上的最大增长方向==，而它的大小代表了函数在该点上的增长率。

* 如果目标是最小化函数，那么梯度指向的方向就是函数值下降最快的方向，我们可以沿着梯度的负方向移动参数，以降低函数的值。如果目标是最大化函数，那么梯度指向的方向就是函数值上升最快的方向。



#### （2）参数更新规则

*参数更新规则定义了参数如何根据梯度进行更新。其中最基本的参数更新规则是梯度下降（Gradient Descent）。*



* 在梯度下降中，参数ω的更新规则如下：
  $$
  ω = ω - α▽ωJ(ω)
  $$
  其中，α是学习率（Learning Rate），▽ωJ(ω) 是目标函数J(ω)对参数ω的梯度

  

* 梯度下降的原理是在每次迭代中更新参数，使得目标函数J(ω)在参数ω的附近下降最快。



## 二 一般步骤

1. 定义目标函数：确定要优化的目标函数，通常是一个损失函数或成本函数，它能够衡量参数取值的好坏。例如，均方误差（Mean Square Error）是常用的损失函数。
2. 初始化参数：为模型的参数赋予初始值。这些参数是需要通过优化来学习的变量。
3. 计算梯度：使用目标函数对参数进行偏导数运算，得到每个参数的梯度值。梯度表示函数在当前参数取值处的变化方向和变化率。
4. 更新参数：根据梯度的反方向对参数进行更新。常见的更新方法是使用学习率（learning rate）乘以梯度，然后将结果加到当前参数上。学习率决定了每次更新的步长大小。
5. 重复迭代：重复步骤3和步骤4，直到满足停止条件。停止条件可以是==达到指定的迭代次数==，梯度==变化小于某个阈值==，或==达到预定的性能指标==等。
6. 得到最优参数：当优化过程结束时，得到的参数值被认为是目标函数的最优解，它使得目标函数取得最小（或最大）值。



## 三 梯度搜索优化算法

### 1. 梯度下降法

#### （1）原理

* *通过不断更新参数来降低目标函数的值*



#### （2）根据更新参数的方式划分

* 批量梯度下降法（Batch Gradient Descent）
* 随机梯度下降法（Stochastic Gradient Descent）
* 小批量梯度下降法（Mini-batch Gradient Descent）

